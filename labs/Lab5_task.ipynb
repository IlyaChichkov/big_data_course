{"cells":[{"cell_type":"markdown","metadata":{"id":"_DsfDI6DqBtO"},"source":["# Лабораторная работа № 5. Решение задачи классификации на примере прогноза состояния системы на основе данных о состоянии ее компонентов."]},{"cell_type":"markdown","metadata":{"id":"VmbTnG0-qBtc"},"source":["В работе проводится ознакомление с различными методами машинного обучения с учителем, решающих задачу классификации. СОздаются различные линейные и нелинейные модели и оценивается точность из прогноза."]},{"cell_type":"markdown","metadata":{"id":"aV2fRv3aqBtg"},"source":["## Введение"]},{"cell_type":"markdown","metadata":{"id":"EjyHwi_EqBti"},"source":["Современные радиолокационные станции (РЛС) – это структурно-сложные радиотехнические и информационные системы, характеризующиеся высокой надежностью функционирования и большим числом цифровых компонентов в своем составе. Одним из таких компонентов является блок усиления мощности (БУМ), задача которого усиливать передаваемый или принимаемый сигнал.\n","\n","Функционирование БУМ приводит к их нагреву, что может сказаться на снижении их работоспособности или даже привести к отказу. Под системой в этой работе мы будем понимать несколько БУМ, объединенных в единое целое. Тогда техническое состояние всей системы будет определяться техническим состоянием ее компонент, т.е. состоянием БУМ в данной работе. Техническое же состояние БУМ напрямую зависит от их температуры: при достижении определенного порога блок перестает работать и начинает охлаждаться. После охлаждения до определенной температуры он снова переходит в рабоспособное состояние.\n","\n","Основная задача - спрогнозировать увеличение температуры блоков усиления мощности на основании истории их функционирования и режима работы блоков, который задает интенсивность нагрева, и возможный выход из строя всей системы блоков. В лабораторной работе № 3 проводится статистический анализ данных тепловой нагрузки модельных БУМ, определяются пороговые значения температур, при которых происходит отключение блоков с целью их охлаждения. На основании пороговых температур вычислено состояние блоков в интервале \\[0, 1\\] и установлен простой критерий определения состояния системы - снижение среднего состояния всех блоков ниже определенного порогового значения. \n","\n","В данной лабораторной работе будут применены различные методы машинного обучения с учителем для установления зависимости состояния системы от состояний блоков и прогноза состояния системы."]},{"cell_type":"markdown","metadata":{"id":"_S9KJFqHqBtm"},"source":["## Описание исходных данных"]},{"cell_type":"markdown","metadata":{"id":"u4j3rBTXqBto"},"source":["Подключим стандартные пакеты для работы с данными и построения графиков"]},{"cell_type":"code","execution_count":78,"metadata":{"id":"gjwqQJm5qBtq"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"5Ed3QNatqBtw"},"source":["Загрузим файл с данными и выведем на экран первые 5 строк. Получим информацию по каждой колонке."]},{"cell_type":"code","execution_count":79,"metadata":{"id":"sxSFtxYEqBty"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>state1</th>\n","      <th>state2</th>\n","      <th>state3</th>\n","      <th>state4</th>\n","      <th>state5</th>\n","      <th>state6</th>\n","      <th>state7</th>\n","      <th>state8</th>\n","      <th>state9</th>\n","      <th>system_state</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.990954</td>\n","      <td>0.996330</td>\n","      <td>1.000000</td>\n","      <td>0.979060</td>\n","      <td>1.000000</td>\n","      <td>0.929844</td>\n","      <td>0.947907</td>\n","      <td>0.952991</td>\n","      <td>0.962632</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.838641</td>\n","      <td>0.806637</td>\n","      <td>0.820733</td>\n","      <td>0.813443</td>\n","      <td>0.797077</td>\n","      <td>0.736372</td>\n","      <td>0.720410</td>\n","      <td>0.780524</td>\n","      <td>0.794755</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.821185</td>\n","      <td>0.769370</td>\n","      <td>0.830724</td>\n","      <td>0.830488</td>\n","      <td>0.813958</td>\n","      <td>0.753848</td>\n","      <td>0.715018</td>\n","      <td>0.781899</td>\n","      <td>0.796795</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.687686</td>\n","      <td>0.604382</td>\n","      <td>0.676615</td>\n","      <td>0.708904</td>\n","      <td>0.624583</td>\n","      <td>0.638659</td>\n","      <td>0.576266</td>\n","      <td>0.615852</td>\n","      <td>0.651636</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.583539</td>\n","      <td>0.503571</td>\n","      <td>0.578079</td>\n","      <td>0.586587</td>\n","      <td>0.534546</td>\n","      <td>0.551319</td>\n","      <td>0.487111</td>\n","      <td>0.529548</td>\n","      <td>0.518788</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     state1    state2    state3    state4    state5    state6    state7  \\\n","0  0.990954  0.996330  1.000000  0.979060  1.000000  0.929844  0.947907   \n","1  0.838641  0.806637  0.820733  0.813443  0.797077  0.736372  0.720410   \n","2  0.821185  0.769370  0.830724  0.830488  0.813958  0.753848  0.715018   \n","3  0.687686  0.604382  0.676615  0.708904  0.624583  0.638659  0.576266   \n","4  0.583539  0.503571  0.578079  0.586587  0.534546  0.551319  0.487111   \n","\n","     state8    state9  system_state  \n","0  0.952991  0.962632           1.0  \n","1  0.780524  0.794755           1.0  \n","2  0.781899  0.796795           1.0  \n","3  0.615852  0.651636           1.0  \n","4  0.529548  0.518788           1.0  "]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(\"Lab5_data.csv\")\n","df.head(5)"]},{"cell_type":"code","execution_count":80,"metadata":{"id":"tKlxfuswqBt1"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1205950 entries, 0 to 1205949\n","Data columns (total 10 columns):\n"," #   Column        Non-Null Count    Dtype  \n","---  ------        --------------    -----  \n"," 0   state1        1205950 non-null  float64\n"," 1   state2        1205950 non-null  float64\n"," 2   state3        1205950 non-null  float64\n"," 3   state4        1205950 non-null  float64\n"," 4   state5        1205950 non-null  float64\n"," 5   state6        1205950 non-null  float64\n"," 6   state7        1205950 non-null  float64\n"," 7   state8        1205950 non-null  float64\n"," 8   state9        1205950 non-null  float64\n"," 9   system_state  1205950 non-null  float64\n","dtypes: float64(10)\n","memory usage: 92.0 MB\n"]}],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"id":"VgrvAO1VqBt2"},"source":["Колонки **state1 - state9** содержат состояние блока 1 - 9 в виде вещественного числа в интервале \\[0, 1\\]. При этом значению 1 соответствует работоспособное состояние с минимальной температурой, в состоянию 0 - выключенное состояние, когда блок находится в режиме обхлаждения. Колонка **system_state** обозначает состояние системы: 1 - работоспособна, 0 - нерабоспособна. Все колонки имеют тип **float64**."]},{"cell_type":"markdown","metadata":{"id":"mt-LyBPmqBt4"},"source":["## Подготовка данных"]},{"cell_type":"markdown","metadata":{"id":"uIKiNU2qqBt5"},"source":["Для использования моделей машинного обучения с учителем необходимо специальным образом подготовить данные: сформировать обучающую выборку, на которой модель будет \"учиться\", т.е. подстраивать свои внутренние параметры, тестовую выборку, на которой будет определяться точность модели в процессе ее обучения, а также валидационную выборку, на которой проверяется итогое качество работы модели. \n","\n","Вместо выделения валидационной выборки можно использовать механизм кросс-валидации.В основе метода лежит разделение исходного множества данных на **k** примерно равных блоков, например 5. Затем на **k-1**, т.е. на 4-х блоках, производится обучение модели, а 5-й блок используется для тестирования. Процедура повторяется **k** раз, при этом на каждом проходе для проверки выбирается новый блок, а обучение производится на оставшихся.\n","![Cross-validation](https://wiki.loginom.ru/images/cross-validation.svg)"]},{"cell_type":"markdown","metadata":{"id":"L-clFbuQqBt7"},"source":["Кросс-валидация имеет два основных преимущества перед применением одного множества для обучения и одного для тестирования модели:\n","\n","- Распределение классов оказывается более равномерным, что улучшает качество обучения.\n","- Если при каждом проходе оценить выходную ошибку модели и усреднить ее по всем проходам, то полученная оценка будет более достоверной.\n","\n","В дальнейшем в этой лабораторной работе будем использовать разбиение на 5 блоков с помощью метода **[KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html?highlight=k%20fold#sklearn.model_selection.KFold 'KFold')**."]},{"cell_type":"code","execution_count":81,"metadata":{"id":"kbvqQcwKqBt9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fold 1\n","Train index:  [      0       1       2 ... 1205945 1205948 1205949]\n","Test index:  [      5      26      28 ... 1205934 1205946 1205947]\n","Fold 2\n","Train index:  [      0       1       2 ... 1205947 1205948 1205949]\n","Test index:  [      8      20      21 ... 1205919 1205921 1205935]\n","Fold 3\n","Train index:  [      0       1       3 ... 1205946 1205947 1205949]\n","Test index:  [      2       4       6 ... 1205944 1205945 1205948]\n","Fold 4\n","Train index:  [      0       1       2 ... 1205947 1205948 1205949]\n","Test index:  [      9      14      15 ... 1205930 1205933 1205938]\n","Fold 5\n","Train index:  [      2       4       5 ... 1205946 1205947 1205948]\n","Test index:  [      0       1       3 ... 1205942 1205943 1205949]\n"]}],"source":["from sklearn.model_selection import KFold\n","\n","kf = KFold(n_splits=5, shuffle=True)\n","X = df.loc[:, 'state1':'state9']\n","y = df['system_state'].astype(int)\n","\n","for i, (train_index, test_index) in enumerate(kf.split(X), start=1):\n","    print('Fold {}'.format(i))\n","    print('Train index: ', train_index)\n","    print('Test index: ', test_index)"]},{"cell_type":"markdown","metadata":{"id":"EJuzMzGlqBt_"},"source":["## Линейные модели машинного обучения"]},{"cell_type":"markdown","metadata":{"id":"_LeJCG4YqBt_"},"source":["Задача определения состояния системы по известным состояниям блоков является задачей бинарной классификации. Среди линейных моделей будем использовать линейную регрессию, линейную регрессию с L1 и L2-регуляризацией, а также логистическую регрессию. Подробное описание работы этих моделей можно прочитать на сайте [Scikit Learn](https://scikit-learn.org/stable/modules/linear_model.html 'Scikit Learn')."]},{"cell_type":"markdown","metadata":{"id":"4q9msIwFqEL8"},"source":["#### **Задание 1** "]},{"cell_type":"markdown","metadata":{"id":"gR_4Iew-qBuB"},"source":["Сделаем процесс обучения различных моделей универсальным. Для этого напишем функцию **regr_accuracy(y_pred, y_test)**, которая будет считать точность спрогнизорованных значений целевой переменной для модели регрессии, функцию **class_accuracy(y_pred, y_test)**, которая будет считать точность спрогнизорованных значений целевой переменной для модели классификации, и функцию **train(model, model_name, evaluate, kfold, X, y)**, которая обучает заданную модель **model** с использованием механизма кросс-валидации **kfold**.\n","\n","Точность - относительная доля правильно спрогнозированных значений."]},{"cell_type":"code","execution_count":98,"metadata":{"id":"5XlMFRASqBuC"},"outputs":[],"source":["from sklearn.metrics import mean_absolute_error, accuracy_score\n","\n","def regr_accuracy(y_pred, y_test):\n","    # y_pred - прогнозные значения\n","    # y_test - истинные значения\n","    # для модели регрессии\n","    return np.sum(y_pred == y_test) /len(y_test)\n","\n","def class_accuracy(y_pred, y_test):\n","    # y_pred - прогнозные значения\n","    # y_test - истинные значения\n","    # для модели классификации\n","    return np.sum(y_pred == y_test) /len(y_test)\n","\n","def train(model, model_name, evaluate, kfold, X, y):\n","    # model - модель для прогноза, обладающая методами fit(), predict()\n","    # model_name - название модели, строковый тип\n","    # evaluate - функция для расчета точности, например функция regr_accuracy() или class_accuracy()\n","    # kfold - объект KFold\n","    # X - признаки\n","    # y - целевая переменная\n","    print('Train model: '+model_name)\n","\n","    scores = []\n","    for train_index, test_index in kfold.split(X):\n","        \n","        X_train, X_test = X.values[train_index], X.values[test_index]\n","        y_train, y_test = y.values[train_index], y.values[test_index]\n","        \n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test).astype(int)\n","\n","        scores.append(evaluate(y_test, y_pred))\n","        \n","        \n","    mean_score = np.mean(scores)\n","    print('Mean score = {:.5f}'.format(mean_score))\n","    return mean_score"]},{"cell_type":"markdown","metadata":{"id":"B5yEVTk8qBuE"},"source":["### Линейная регрессия"]},{"cell_type":"markdown","metadata":{"id":"o1LF2lQXqBuF"},"source":["В линейных моделях целевая переменная $\\hat{y}$ определяется как линейная комбинация известных переменных (признаков):\n","\n","$$\\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p$$\n","\n","В модели линейной регрессии коэффициенты $w = (w_1, ..., w_p)$ подбираются таким образом, чтобы минимизировать сумму квадратов отклонений рассчитанных значений целевой переменной от истинных значений:\n","\n","$$\\min_{w} || X w - y||_2^2$$\n","\n","Важно отметить, что линейные модели чувствительны к абсолютным значениям признаков, поэтому следует перед применением линейных моделей провести нормирование исходных данных (обычно на интервал \\[0,1\\]). Также применение линейных моделей основано на предположении о линейной независимости признаков, поэтому следует стараться не использовать в качестве признаков коррелированные признаки. В противном случае модель будет чувствительна к шумам, т.е. случайным выбросам в значениях признаков."]},{"cell_type":"markdown","metadata":{"id":"ZrfRKpNhqBuG"},"source":["Создадим и обучим модель **[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression 'LinearRegression')**. Запишем точность модели в словарь **scores**."]},{"cell_type":"code","execution_count":99,"metadata":{"id":"t1yueQNJqBuH"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train model: linear regression\n","Mean score = 0.68159\n"]}],"source":["from sklearn import linear_model\n","\n","lin_reg = linear_model.LinearRegression()\n","\n","scores = dict()\n","\n","score = train(lin_reg, 'linear regression', regr_accuracy, kf, X, y)\n","scores['linear regression'] = score"]},{"cell_type":"markdown","metadata":{"id":"8SOVicj5qBuI"},"source":["### Линейная регрессия c L1 и L2 регуляризацией"]},{"cell_type":"markdown","metadata":{"id":"DGNWt1qDqBuJ"},"source":["Если размер обучающей выборки невелик, а число признаков, наоборот, достаточно велико, то коэффициенты модели могут быть подобраны таким образом, чтобы модель максимально точно учитывала все точки из обучающей выборки, при этом вне обучающей выборки модель будет давать большую ошибку. Это явление носит название переобучения. Одним из способов препятствовать переобучению является механизмы регуляризации. Он ограничивает значения коэффицентов $w = (w_1, ..., w_p)$, используемых в модели.\n","\n","L1-регуляризация вносит дополнительный \"штраф\", пропорциональный модулю значения коэффициента:\n","$$\\min_{w} ||X w - y||_2 ^ 2 + \\alpha ||w||_1$$\n","\n","L2-регуляризация вносит дополнительный \"штраф\", пропорциональный квадрату модуля значения коэффициента:\n","$$\\min_{w} || X w - y||_2^2 + \\alpha ||w||_2^2$$\n","\n","Параметр $\\alpha$ задает \"силу\" регуляризации. L1-регуляризация приведет к тому, что все несущественные признаки будут иметь вес, равный 0. L2-регуляризация приведет к тому, что несущественные признаки будут иметь околонулевые веса. Продемонстрируем это на примере. Создадим и обучим модели **[Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso 'Lasso')** (L1-регуляризация) и **[Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge 'Ridge')** (L2-регуляризация).\n","\n","Подберите оптимальное значения параметра регуляризации **$\\alpha$** для модели **Lasso**."]},{"cell_type":"markdown","metadata":{"id":"WA9AmWF9qRZ4"},"source":["#### **Задание 2** "]},{"cell_type":"code","execution_count":100,"metadata":{"id":"BbZqmJWUqBuJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Alpha = 0.0001\n","Coefficients:  [0.16542942 0.16946404 0.16572914 0.16082697 0.16851875 0.16864115\n"," 0.16507533 0.16953335 0.17136388]\n","Alpha = 0.001\n","Coefficients:  [0.16326228 0.16731439 0.16351531 0.15859281 0.16649824 0.16657605\n"," 0.16306425 0.16737437 0.16928289]\n","Alpha = 0.01\n","Coefficients:  [0.14159091 0.14581789 0.14137704 0.13625125 0.1462931  0.145925\n"," 0.14295352 0.14578453 0.14847301]\n","Alpha = 0.1\n","Coefficients:  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","Alpha = 1\n","Coefficients:  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"]}],"source":["for alpha in [1e-4, 1e-3, 0.01, 0.1, 1]:\n","    print('Alpha = {}'.format(alpha))\n","    lasso = linear_model.Lasso(alpha)\n","    lasso.fit(X, y)\n","    # напишите здесь ваш код\n","    \n","    print('Coefficients: ', lasso.coef_)"]},{"cell_type":"markdown","metadata":{"id":"bEtMb7HnqBuK"},"source":["Обучим модель с оптимальным значением параметра $\\alpha$. Сохраним точность прогноза в словаре **scores**."]},{"cell_type":"code","execution_count":101,"metadata":{"id":"ijIaEpdmqBuK"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train model: lasso\n","Mean score = 0.68141\n"]}],"source":["lasso = linear_model.Lasso(alpha = 0.0001)\n","score = train(lasso, 'lasso', regr_accuracy, kf, X, y)\n","scores['lasso'] = score"]},{"cell_type":"markdown","metadata":{"id":"lT6aVlFvqUWP"},"source":["#### **Задание 3** "]},{"cell_type":"markdown","metadata":{"id":"jC4eciIHqBuL"},"source":["Подберите оптимальное значения параметра регуляризации **$\\alpha$** для модели **Ridge**."]},{"cell_type":"code","execution_count":103,"metadata":{"id":"NbCQ_NzuqBuM"},"outputs":[{"name":"stdout","output_type":"stream","text":["Alpha = 0.0001\n","Coefficients:  [0.16568775 0.16971513 0.16597877 0.16106906 0.16872992 0.16885621\n"," 0.16528861 0.16977101 0.17160088]\n","Alpha = 0.001\n","Coefficients:  [0.16568775 0.16971513 0.16597877 0.16106906 0.16872992 0.16885621\n"," 0.16528861 0.16977101 0.17160088]\n","Alpha = 0.01\n","Coefficients:  [0.16568774 0.16971513 0.16597877 0.16106906 0.16872992 0.1688562\n"," 0.16528861 0.169771   0.17160088]\n","Alpha = 0.1\n","Coefficients:  [0.16568772 0.1697151  0.16597874 0.16106904 0.16872989 0.16885617\n"," 0.16528858 0.16977097 0.17160084]\n","Alpha = 1\n","Coefficients:  [0.16568744 0.16971477 0.16597845 0.16106882 0.16872959 0.16885586\n"," 0.16528833 0.16977064 0.17160049]\n"]}],"source":["from sklearn.linear_model import Ridge\n","\n","\n","for alpha in [1e-4, 1e-3, 0.01, 0.1, 1]:\n","    print('Alpha = {}'.format(alpha))\n","    ridge = linear_model.Ridge(alpha)\n","    ridge.fit(X.values, y.values)\n","    # напишите здесь ваш код\n","    \n","    print('Coefficients: ', ridge.coef_)"]},{"cell_type":"markdown","metadata":{"id":"Mx1E56OwqBuM"},"source":["Обучим модель с оптимальным значением параметра $\\alpha$. Сохраним точность прогноза в словаре **scores**."]},{"cell_type":"code","execution_count":104,"metadata":{"id":"mvicDflUqBuM"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train model: ridge\n","Mean score = 0.68161\n"]}],"source":["ridge = linear_model.Ridge(alpha = 0.0001)\n","score = train(ridge, 'ridge', regr_accuracy, kf, X, y)\n","scores['ridge'] = score"]},{"cell_type":"markdown","metadata":{"id":"V18B9WFqqBuN"},"source":["Как видим, **Lasso** просто занулила все коэффициенты при $\\alpha>0.01$."]},{"cell_type":"markdown","metadata":{"id":"_bm3DSxiqBuN"},"source":["### Логистическая регрессия"]},{"cell_type":"markdown","metadata":{"id":"Zk3mvsA_qBuO"},"source":["Если на выходе линейной регрессии получается просто вещественное число, то в логистической регрессии это число преобразуется с помощью логистической функции в отрезок \\[0,1\\], а потому может трактоваться как вероятность получения на выходе дискретного значения 1. \n","\n","$$f(y)=\\dfrac{1}{1+e^{-y}}$$\n","\n","Таким образом, модель логистической регрессии может успешно использоваться как бинарный классификатор. Логистическая регрессия минимизирует следующую величину (L1 и L2 регуляризация уже включены и контролируются параметрами $C$ - \"сила\" регуляризации (малые значения - \"сильная\" регуляризация), $\\rho$ - относительный вклад L1-регуляризации):\n","\n","$$\\min_{w, c} \\frac{1 - \\rho}{2}w^T w + \\rho \\|w\\|_1 + C \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1)$$\n","\n","Создадим модель **[LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression 'Logistic Regression')** и обучим ее.\n","\n","Подберите оптимальное значение параметра регуляризации **С** и тип регуляризации **penalty**. "]},{"cell_type":"markdown","metadata":{"id":"N5CGOf2GqeeK"},"source":["#### **Задание 4** "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1P4VdrGSqBuP"},"outputs":[{"name":"stdout","output_type":"stream","text":["C = 0.0001\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Ilya\\Documents\\big_data_course\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n","  warnings.warn(\n","c:\\Users\\Ilya\\Documents\\big_data_course\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Coefficients:  [[1.29326641 1.31510127 1.2923988  1.26167494 1.30480954 1.29970724\n","  1.27928605 1.32487021 1.33150372]]\n","C = 0.001\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Ilya\\Documents\\big_data_course\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n","  warnings.warn(\n","c:\\Users\\Ilya\\Documents\\big_data_course\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Coefficients:  [[1.29326641 1.31510127 1.2923988  1.26167494 1.30480954 1.29970724\n","  1.27928605 1.32487021 1.33150372]]\n","C = 0.01\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Ilya\\Documents\\big_data_course\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n","  warnings.warn(\n","c:\\Users\\Ilya\\Documents\\big_data_course\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Coefficients:  [[1.29326641 1.31510127 1.2923988  1.26167494 1.30480954 1.29970724\n","  1.27928605 1.32487021 1.33150372]]\n","C = 0.1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Ilya\\Documents\\big_data_course\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n","  warnings.warn(\n","c:\\Users\\Ilya\\Documents\\big_data_course\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Coefficients:  [[1.29326641 1.31510127 1.2923988  1.26167494 1.30480954 1.29970724\n","  1.27928605 1.32487021 1.33150372]]\n","C = 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Ilya\\Documents\\big_data_course\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Coefficients:  [[1.29326641 1.31510127 1.2923988  1.26167494 1.30480954 1.29970724\n","  1.27928605 1.32487021 1.33150372]]\n","C = 0.0001\n","Coefficients:  [[0.94428564 0.95527404 0.9428829  0.92846392 0.95544762 0.95217514\n","  0.94471166 0.957817   0.96358774]]\n","C = 0.001\n","Coefficients:  [[1.23401978 1.25356599 1.23296359 1.2058224  1.24583208 1.24106652\n","  1.22359951 1.26161396 1.26817165]]\n","C = 0.01\n","Coefficients:  [[1.28680973 1.30838514 1.28591942 1.25560501 1.29838991 1.29332568\n","  1.27324397 1.31795508 1.32458075]]\n","C = 0.1\n","Coefficients:  [[1.29261472 1.31442328 1.29174479 1.26106248 1.30416168 1.29906324\n","  1.27867651 1.32417199 1.33080472]]\n","C = 1\n","Coefficients:  [[1.29320118 1.31503341 1.29233334 1.26161364 1.30474469 1.29964278\n","  1.27922504 1.32480032 1.33143375]]\n","C = 0.0001\n"]},{"ename":"ValueError","evalue":"Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[73], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mC = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(c))\n\u001b[0;32m      7\u001b[0m log_reg \u001b[39m=\u001b[39m LogisticRegression(C\u001b[39m=\u001b[39mc, penalty\u001b[39m=\u001b[39mpenalty)\n\u001b[1;32m----> 8\u001b[0m log_reg\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCoefficients: \u001b[39m\u001b[39m'\u001b[39m, log_reg\u001b[39m.\u001b[39mcoef_)\n","File \u001b[1;32mc:\\Users\\Ilya\\Documents\\big_data_course\\.venv\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[1;32mc:\\Users\\Ilya\\Documents\\big_data_course\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1169\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1139\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1141\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m \u001b[39m    Fit the model according to the given training data.\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1167\u001b[0m \u001b[39m    The SAGA solver supports both float64 and float32 bit arrays.\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1169\u001b[0m     solver \u001b[39m=\u001b[39m _check_solver(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msolver, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpenalty, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdual)\n\u001b[0;32m   1171\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpenalty \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39melasticnet\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml1_ratio \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1172\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1173\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39ml1_ratio parameter is only used when penalty is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1174\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39melasticnet\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. Got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1175\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m(penalty=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpenalty)\n\u001b[0;32m   1176\u001b[0m         )\n","File \u001b[1;32mc:\\Users\\Ilya\\Documents\\big_data_course\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:56\u001b[0m, in \u001b[0;36m_check_solver\u001b[1;34m(solver, penalty, dual)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_solver\u001b[39m(solver, penalty, dual):\n\u001b[0;32m     54\u001b[0m     \u001b[39m# TODO(1.4): Remove \"none\" option\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[39mif\u001b[39;00m solver \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mliblinear\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msaga\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mand\u001b[39;00m penalty \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39ml2\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 56\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     57\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSolver \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m supports only \u001b[39m\u001b[39m'\u001b[39m\u001b[39ml2\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m\u001b[39m penalties, got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m penalty.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     58\u001b[0m             \u001b[39m%\u001b[39m (solver, penalty)\n\u001b[0;32m     59\u001b[0m         )\n\u001b[0;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m solver \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mliblinear\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m dual:\n\u001b[0;32m     61\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     62\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSolver \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m supports only dual=False, got dual=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (solver, dual)\n\u001b[0;32m     63\u001b[0m         )\n","\u001b[1;31mValueError\u001b[0m: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty."]}],"source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","\n","for penalty in ['none', 'l2']:\n","    for c in [1e-4, 1e-3, 0.01, 0.1, 1]:\n","        print('C = {}'.format(c))\n","        log_reg = LogisticRegression(C=c, penalty=penalty)\n","        log_reg.fit(X, y)\n","        \n","        print('Coefficients: ', log_reg.coef_)"]},{"cell_type":"markdown","metadata":{"id":"IW1aI40TqBuP"},"source":["Обучим модель с оптимальным значением параметра **С**. Сохраним точность прогноза в словаре **scores**."]},{"cell_type":"code","execution_count":105,"metadata":{"id":"cctaYPAcqBuP"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train model: logistic regression\n","Mean score = 0.83742\n"]}],"source":["logistic_regr = linear_model.LogisticRegression(penalty = 'l2', C=1, solver='lbfgs')\n","score = train(logistic_regr, 'logistic regression', class_accuracy, kf, X, y)\n","scores['logistic regression'] = score"]},{"cell_type":"markdown","metadata":{"id":"wYfkTsd0qBuQ"},"source":["## Метод опорных векторов"]},{"cell_type":"markdown","metadata":{"id":"NdFpL7e3qBuQ"},"source":["Этот метод применим для решения как задач классификации, так и регрессии, и кластеризации. Основными достоинствами метода являются:\n","\n","- эффективность при большой размерности пространства признаков\n","- в процессе обучения запоминается только подвыборка обучающей выборки - опорные вектора, т.е. требует меньший объем памяти\n","- можно применять разные ядра (kernels) для формирования модели\n","\n","Недостатком метода опорных векторов является то, что в случае, когда размерность пространства признаков много больше объема обучающей выборки, на результат работы модели сильно влияет выбор ядра. Также этот метод не позволяет быстро и просто получить вероятность прогноза.\n","\n","С математической точки зрения, метод опорных векторов проводит гипер-плоскость, которая разделяет один класс от другого. При этом граница проводится так, что быть расположенной максимально далеко от каждой из точек.\n","![SVC](https://scikit-learn.org/stable/_images/sphx_glr_plot_separating_hyperplane_0011.png)\n","\n","Функция ядра определяет, какие признаки будут использоваться в качестве переменных в гиперпространстве, в котором проводится гипер-плоскость. Например, для линейного ядра $\\langle x, x'\\rangle$ берутся исходные признаки, для полиномиального ядра - полиномы от исходных признаков $(\\gamma \\langle x, x'\\rangle + r)^d$, для radial-basis-function (rbf) - экспоненциальная функция $\\exp(-\\gamma \\|x-x'\\|^2)$.\n","\n","Построим модель Support Vector Classifier - [**SVC**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC 'SVC') для различных ядер. Заранее уменьшим размер выборки, что позволит проводить обучения в разумное время (метод опорных векторов довольно долго обучается).\n","\n","Определите оптимальное значение параметра регуляризации **С** и типа ядра **kernel**."]},{"cell_type":"markdown","metadata":{"id":"ivF5OJxVqhSq"},"source":["#### **Задание 5**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r0bgmnaoqBuR"},"outputs":[],"source":["from sklearn.svm import SVC\n","X_svc = X.iloc[:10000, :]\n","y_svc = y[:10000]\n","\n","# напишите здесь ваш код\n","kernel = ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']"]},{"cell_type":"markdown","metadata":{"id":"sfsLap6cqBuR"},"source":["Обучим модель с оптимальным значением параметра **С** и типом ядра **kernel**. Сохраним точность прогноза в словаре **scores**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"scitVGNbqBuS"},"outputs":[],"source":["svc = SVC(C=0.001, kernel=, gamma='auto')\n","score = train(svc, 'svc', class_accuracy, kf, X_svc, y_svc)\n","scores['svc'] = score"]},{"cell_type":"markdown","metadata":{"id":"VNehNP8gqBuS"},"source":["## Дерево решений"]},{"cell_type":"markdown","metadata":{"id":"UP-bW4hwqBuS"},"source":["В модели дерева решений (Decision Tree) в процессе обучения строится алгоритм, по которому выполняется прогноз модели. При этом алгоритм представляет из себя дерево, каждый лист которого - это проверка на то, что какой-либо признак из обучающей выборки принимает определенное значение. Пример дерева решений приведен на рисунке ниже.\n","![Decision Tree](https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_dtc_0021.png)\n","\n","Преимуществами такого метода являются:\n","- простота визуализации и хорошая интерпретируемость алгоритма прогноза\n","- не требуется нормализация данных\n","- скорость прогноза пропорциональна логарифму объема выборки, т.е. этот метод быстрый\n","- может обрабатывать как числовые, так и категориальные данные\n","\n","Недостатками метода являются:\n","- деревья легко переобучаются\n","- небольшие изменения в обучающей выборке могут привести к перестойке всего дерева, т.е. метод нестабилен\n","- предсказания деревьев являются кусочно-постоянными, поэтому не годятся для экстраполирования\n","- требуется сбалансировать обучающую выборку по классам, чтобы не допустить \"перекоса\" дерева в сторону какого-либо класса\n","\n","Конкретную математическую реализалицаю алгоритма построения дерева решений можно изучить, например, [здесь.](https://scikit-learn.org/stable/modules/tree.html)\n","\n","Создадим модель [**DecisionTreeClassifier**](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier 'DecisionTreeClassifier') и обучим ее. Для того, чтобы предотвратить переобучение дерева, обычно ограничивается максимальная глубина дерева - параметр **max_depth**, а также минимальное число элементов из обучающей выборки, приходящееся на определнный лист, чтобы можно было с него сделать новое ветвление - параметр **min_samples_split**.\n","\n","Подберите оптимальное значение параметров **max_depth** и **min_samples_split**."]},{"cell_type":"markdown","metadata":{"id":"3dc10J6-qq_P"},"source":["#### **Задание 6** "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DSShPADxqBuT"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","# напишите здесь ваш код"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQExOWvOqBuT"},"outputs":[],"source":["# напишите здесь ваш код"]},{"cell_type":"markdown","metadata":{"id":"NJB7z7RiqBuU"},"source":["Создадим модель с оптимальными значениями параметров **max_depth** и **min_samples_split**. Добавим ее в словарь **scores**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STjh7rAuqBuU"},"outputs":[],"source":["dtc = DecisionTreeClassifier(max_depth=, min_samples_split=, min_samples_leaf=1, random_state=0)\n","score = train(dtc, 'decision tree', class_accuracy, kf, X, y)\n","scores['decision tree'] = score"]},{"cell_type":"markdown","metadata":{"id":"L54zzd7KqBuU"},"source":["## Сравнение различных моделей"]},{"cell_type":"markdown","metadata":{"id":"xEaMPqy-qBuU"},"source":["Отобразим на графике точность работы каждой построенной модели. Для этого будем использовать значения из словаря **scores**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b6SRzljXqBuV"},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","xx = list(scores.keys())\n","yy = list(scores.values())\n","rects = plt.bar(xx, yy)\n","plt.ylim(0.7, 0.9)\n","plt.ylabel('Accuracy')\n","for i, rect in enumerate(rects):\n","    yloc = rect.get_height()\n","    xloc = rect.get_x() + rect.get_width() / 4\n","    plt.annotate(round(yy[i], 4), xy=(xloc, yloc), xytext=(xloc, 10),\n","                            textcoords=\"offset points\",\n","                            va='center',\n","                            color='black', clip_on=True)"]},{"cell_type":"markdown","metadata":{"id":"mFqOwK_mqBuV"},"source":["# Выводы"]},{"cell_type":"markdown","metadata":{"id":"u_haAZPuqwqf"},"source":["#### **Задание 7** "]},{"cell_type":"markdown","metadata":{"id":"XXjkE-WUqBuV"},"source":["Напишите выводы по лабораторной работе"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BfKKrNttqBuW"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
